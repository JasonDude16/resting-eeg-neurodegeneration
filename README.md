# resting-eeg-neurodegeneration

## Overview
For this project I analyzed electroencephalography (EEG) resting state-closed eyes recordings from subjects with Alzheimer's Disease (N=36), Frontotemporal Dementia (N=23), and healthy controls (N=29), with the goal of predicting which group the subject belongs to using spectral features extracted from the EEG recordings. 

These data are publicly available on Kaggle (https://www.kaggle.com/datasets/yosftag/open-nuro-dataset/data), and the corresponding publication can be found at https://doi.org/10.3390/data8060095.

## Abstract
*Introduction*: Alzheimer's Disease (AD) and Frontotemporal Dementia (FTD) are two devastating neurodegenerative diseases with high economic burden. Although AD and FTD often result in similar clinical manifestations, their etiologies are distinct, and likely require different clinical interventions. Models that can differentiate between AD, FTD, and cognitively normal subjects (CN) with high accuracy would allow for early intervention and appropriate treatment of these neurodegenerative diseases.

*Methods*: Participants were diagnosed with AD (N=36), FTD (N=23), or were in the CN group (N=29). Cognitive and neuropsychological state was evaluated by the International Mini-Mental State Examination (MMSE). Resting state eyes-closed electroencephalography (EEG) recordings were obtained, with 19 scalp electrodes placed according to the 10-20 international system. Data were pre-processed using bandpass filtering (0.5-45Hz), artifact subspace reconstruction routine, independent component analysis with automated component rejection, and were then resampled to 128Hz. Bandpower of canonical frequency bands from the 0.5-40Hz range was extracted using Welch's method for channels F3, F4, C3, C4, T3, T4 using all but the first and last minute of the recording. After partitioning the data into train and test sets using stratified random sampling, a support vector machine model (SVM) was trained on 80% of the data using 10-fold cross-validation and a hyperparameter grid search, and then evaluated on the remaining 20% (test set).

*Results*: As expected, groups differed by MMSE (p < .001 for all comparisons; AD: 17.75, FTD: 22.17, CN: 30). There were no age differences between groups (p=0.11; AD: 66.4, FTD: 63.6, CN: 67.9). The SVM model achieved 95% accuracy on 3-class classification when on the train set (N=70) when using the best hyperparameters, however accuracy dropped to 50% when using the test set for evaluation (N=18).

*Discussion*: The SVM model performed well during training but not during testing. This discrepancy betweeen training and testing performance can often be explained by the bias-variance tradeoff, where the model was likely overfitting on the training data, and the model weights did not generalize well to the new data. Another potential contributor to this discrepancy could be the small test set size (and also small train set), which can cause variability in model performance. Nevertheless, machine learning algorithms show promise for differentiating AD, FTD and cognitive normal subjects, as only a small and simple feature set was used to obtain greater-than-chance accuracy.